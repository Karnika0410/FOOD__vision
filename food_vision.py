# -*- coding: utf-8 -*-
"""Food_Vision.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PoHVuFytI1nvI0AUqjGY_hzH7r6MYXBh
"""

import tensorflow as tf
import zipfile
!wget https://storage.googleapis.com/ztm_tf_course/food_vision/101_food_classes_10_percent.zip
zipo=zipfile.ZipFile('101_food_classes_10_percent.zip')
zipo.extractall()
zipo.close()

train_dir='101_food_classes_10_percent/train/'
test_dir='101_food_classes_10_percent/test/'

import os

# Walk through pizza_steak directory and list number of files
for dirpath, dirnames, filenames in os.walk("pizza_steak"):
  print(f"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.")

import numpy as np
import pathlib
data_dir=pathlib.Path('101_food_classes_10_percent/train/')
class_name=np.array(sorted([item.name for item in data_dir.glob('*')]))
print(len(class_name))

from keras.utils import image_dataset
img_size=(224,224)
train_data=tf.keras.preprocessing.image_dataset_from_directory(train_dir,
                                                               image_size=img_size,

                                                               label_mode='categorical')
test_data=tf.keras.preprocessing.image_dataset_from_directory(test_dir,
                                                              image_size=img_size,

                                                              label_mode='categorical',
                                                               shuffle=False)

import datetime
def create_tensorboard_callback(dir_name,expirement_name):
  log_dir =dir_name +'/'+experiment_name+'/'+ datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
  tensorboard_callback = tf.keras.callbacks.TensorBoard(
      log_dir=log_dir
  )
  return tenosorboard_callback

checkpoint_path ="101_food_class_checkpoints_weight/checkpoint.ckpt"
checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                       save_weights_only=True,
                                                       save_best_only=True,
                                                       monitor='val_accuracy')

from tensorflow.keras import layers
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras.models import Sequential

data_argu= tf.keras.Sequential([
    layers.RandomFlip('horizontal'),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
    layers.RandomHeight(0.2),
    layers.RandomWidth(0.2)],
    name='data_argu'
)
base_model=tf.keras.applications.EfficientNetB0(include_top=False)
base_model.trainable=False
input=layers.Input(shape=(224,224,3),name="input")
x=data_argu(input)
x=base_model(x,training=False)
x=layers.GlobalAveragePooling2D(name="global_average_pool")(x)
outputs=layers.Dense(101,activation="softmax",name="output_layers")(x)
model=tf.keras.Model(input,outputs)

model.summary()

model.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              metrics=["accuracy"])

history = model.fit(train_data,
                    epochs=5,
                    steps_per_epoch=len(train_data),
                    validation_data=test_data,
                    validation_steps=int(0.15 * len(test_data)),
                    callbacks=[checkpoint_callback])

result=model.evaluate(test_data)
result

# Check out our model's training curves
plot_loss_curves(history)

base_model.trainable=True

for layer in base_model.layers[:-5]:
       layer.trainable=False

model.compile(loss="categorical_crossentropy",
              optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),
              metrics=["accuracy"])

fine_tune=10

history1=model.fit(train_data,
                  epochs=fine_tune,
                  steps_per_epoch=len(train_data),
                    validation_data=test_data,
                    validation_steps=int(0.15 * len(test_data)),
                  initial_epoch=history.epoch[-1])